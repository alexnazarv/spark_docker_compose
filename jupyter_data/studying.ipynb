{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0746ee64-983a-4d25-a1d5-ba0e8d1a4b39",
   "metadata": {},
   "source": [
    "#### Launch spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beb5c09b-4a26-4b21-bc5b-1a7a030d5f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.executor.instances\", 2) \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.cores\", \"4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"minio:9010\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"root\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"root12345\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1,com.crealytics:spark-excel_2.12:0.13.5\") \\\n",
    "    .config(\"spark.driver.memory\", \"6g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"3g\") \\\n",
    "    .config(\"spark.deploy.defaultCores\", 1) \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", True) \\\n",
    "    .appName(\"MySparkApp\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34f09d6-01ef-45ab-8c06-0db4e30391f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### RDD programming guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d28027-b6ff-4e4f-b695-da99b50144ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "CPU times: user 19.9 ms, sys: 516 µs, total: 20.4 ms\n",
      "Wall time: 11.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: bigint]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "l = [('Alice', 1)]\n",
    "\n",
    "l = spark.createDataFrame(l, ['name', 'age'])\n",
    "\n",
    "l.collect()\n",
    "\n",
    "print(l.is_cached)\n",
    "l.persist(storageLevel=StorageLevel.MEMORY_AND_DISK)\n",
    "print(l.is_cached)\n",
    "l.count()\n",
    "l.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6baec039-7cdd-4b12-a4b5-3e60397129c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0685d81-841a-41c0-86a9-bd938eec261a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('asdasdas', 2), ('asdr23rsaa', 4), ('asdasdaswe', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = spark.sparkContext.textFile(\"data.txt\")\n",
    "pairs = lines.map(lambda s: (s, 1))\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d067193-b3f7-4b22-8aab-de6ccf5817ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts = pairs.reduce(lambda a, b: a + b)\n",
    "# counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86eeedb9-85a0-4319-bf50-f0bf664e85bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 2), ('c', 12), ('a', 4)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = spark.sparkContext.parallelize([('a', 1), ('b', 2), ('a', 3), ('c', 3), ('c', 4), ('c', 5)])\n",
    "print(l.reduceByKey(lambda x, y: x + y).collect())\n",
    "\n",
    "l = spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "l.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7908abda-6ef1-4d93-b228-cc04671c259c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 1, 'a', 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('a',1) + ('a', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "965ded8b-b3c3-4ade-904f-763cbf3f6d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', 'How', 'are', 'you']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = spark.sparkContext.parallelize([\"Hello world\", \"How are you\"])\n",
    "\n",
    "# Apply flatMap to split each sentence into words\n",
    "words = sentences.flatMap(lambda sentence: sentence.split())\n",
    "\n",
    "# Collect the results\n",
    "result = words.collect()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7ef4adf-efc7-4cad-80c0-4136af467224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = spark.sparkContext.accumulator(0)\n",
    "\n",
    "def g(x):\n",
    "    accum.add(1)\n",
    "    return x + x\n",
    "\n",
    "data = spark.sparkContext.parallelize([1,2,3])\n",
    "\n",
    "data.map(g).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a84f4eb-d2a2-4f4e-abca-f54cf9351ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=0, value=6>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.map(g).collect()\n",
    "accum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c1715f-bedd-403d-ba2c-cab752d2959f",
   "metadata": {},
   "source": [
    "### Data skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71bbcb2-608f-4d29-b9cf-431804887189",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(F.spark_partition_id()).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0faf488a-3feb-43a9-92d2-1cffb8bfc81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Define the size of the DataFrame\n",
    "num_records = 10**5\n",
    "\n",
    "# Generate names\n",
    "names = [fake.name() for _ in range(4)]  # 4 names\n",
    "\n",
    "# Define the distribution of random integers\n",
    "# 85% of random integers correspond to the first name\n",
    "# 15% of random integers are evenly distributed across the other three names\n",
    "int_distribution = [0.85 * num_records] + [0.05 * num_records] * 3\n",
    "\n",
    "# Generate random integers according to the specified distribution\n",
    "random_ints = np.random.choice(len(names), size=num_records, p=[0.85, 0.05, 0.05, 0.05])\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame({'name': [names[i] for i in random_ints],\n",
    "                   'random_int': np.random.randint(1000, 10000, size=num_records)})\n",
    "\n",
    "spark_df = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1172f1d9-a99e-450d-85fe-fbb0f133e448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|SPARK_PARTITION_ID()|count|\n",
      "+--------------------+-----+\n",
      "|                   0|25000|\n",
      "|                   2|25000|\n",
      "|                   1|25000|\n",
      "|                   3|25000|\n",
      "+--------------------+-----+\n",
      "\n",
      "CPU times: user 7.27 ms, sys: 186 µs, total: 7.46 ms\n",
      "Wall time: 6.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark_df = spark_df.repartition(4)\n",
    "spark_df.groupBy(F.spark_partition_id()).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff63fc-f2fe-47cc-94b9-2b1b6268a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "spark_df.repartition(4).write.partitionBy('name').mode('overwrite').parquet('s3a://spark/spark_df.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f72fa3-af34-4081-9478-cfd62f673714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
