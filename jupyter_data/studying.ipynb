{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0746ee64-983a-4d25-a1d5-ba0e8d1a4b39",
   "metadata": {},
   "source": [
    "#### Launch spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beb5c09b-4a26-4b21-bc5b-1a7a030d5f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"spark://spark-master:7077\") \\\n",
    "#     .config(\"spark.executor.instances\", 4) \\\n",
    "#     .config(\"spark.executor.memory\", \"2g\") \\\n",
    "#     .config(\"spark.executor.cores\", \"2\") \\\n",
    "#     .config(\"spark.driver.cores\", \"4\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.access.key\", \"root\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.secret.key\", \"root12345\") \\\n",
    "#     .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "#     .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1,com.crealytics:spark-excel_2.12:0.13.5\") \\\n",
    "#     .config(\"spark.driver.memory\", \"6g\") \\\n",
    "#     .config(\"spark.driver.maxResultSize\", \"3g\") \\\n",
    "#     .config(\"spark.deploy.defaultCores\", 2) \\\n",
    "#     .config(\"spark.dynamicAllocation.enabled\", True) \\\n",
    "#     .appName(\"MySparkApp\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# spark.sparkContext.setLogLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d530349-6ce7-4ebe-8ef7-2aa575b7ed2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spark_session():\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .config(\"spark.executor.instances\", 4) \\\n",
    "        .config(\"spark.executor.memory\", \"2g\") \\\n",
    "        .config(\"spark.executor.cores\", \"2\") \\\n",
    "        .config(\"spark.driver.cores\", \"4\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", \"root\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", \"root12345\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1,com.crealytics:spark-excel_2.12:0.13.5\") \\\n",
    "        .config(\"spark.driver.memory\", \"6g\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"3g\") \\\n",
    "        .config(\"spark.deploy.defaultCores\", 2) \\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True) \\\n",
    "        .appName(\"MySparkApp\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d28027-b6ff-4e4f-b695-da99b50144ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "CPU times: user 19.9 ms, sys: 516 Âµs, total: 20.4 ms\n",
      "Wall time: 11.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: bigint]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "l = [('Alice', 1)]\n",
    "\n",
    "l = spark.createDataFrame(l, ['name', 'age'])\n",
    "\n",
    "l.collect()\n",
    "\n",
    "print(l.is_cached)\n",
    "l.persist(storageLevel=StorageLevel.MEMORY_AND_DISK)\n",
    "print(l.is_cached)\n",
    "l.count()\n",
    "l.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6baec039-7cdd-4b12-a4b5-3e60397129c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0685d81-841a-41c0-86a9-bd938eec261a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('asdasdas', 2), ('asdr23rsaa', 4), ('asdasdaswe', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = spark.sparkContext.textFile(\"data.txt\")\n",
    "pairs = lines.map(lambda s: (s, 1))\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "counts.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d067193-b3f7-4b22-8aab-de6ccf5817ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts = pairs.reduce(lambda a, b: a + b)\n",
    "# counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86eeedb9-85a0-4319-bf50-f0bf664e85bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 2), ('c', 12), ('a', 4)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = spark.sparkContext.parallelize([('a', 1), ('b', 2), ('a', 3), ('c', 3), ('c', 4), ('c', 5)])\n",
    "print(l.reduceByKey(lambda x, y: x + y).collect())\n",
    "\n",
    "l = spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "l.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7908abda-6ef1-4d93-b228-cc04671c259c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 1, 'a', 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('a',1) + ('a', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "965ded8b-b3c3-4ade-904f-763cbf3f6d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', 'How', 'are', 'you']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = spark.sparkContext.parallelize([\"Hello world\", \"How are you\"])\n",
    "\n",
    "# Apply flatMap to split each sentence into words\n",
    "words = sentences.flatMap(lambda sentence: sentence.split())\n",
    "\n",
    "# Collect the results\n",
    "result = words.collect()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7ef4adf-efc7-4cad-80c0-4136af467224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = spark.sparkContext.accumulator(0)\n",
    "\n",
    "def g(x):\n",
    "    accum.add(1)\n",
    "    return x + x\n",
    "\n",
    "data = spark.sparkContext.parallelize([1,2,3])\n",
    "\n",
    "data.map(g).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a84f4eb-d2a2-4f4e-abca-f54cf9351ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=0, value=6>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.map(g).collect()\n",
    "accum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c1715f-bedd-403d-ba2c-cab752d2959f",
   "metadata": {},
   "source": [
    "### Data skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b8c52f-abe8-4845-9537-8691c6a6d168",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Generate big DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0faf488a-3feb-43a9-92d2-1cffb8bfc81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 31s, sys: 1.27 s, total: 1min 32s\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Define the size of the DataFrame\n",
    "num_records = 10**7\n",
    "\n",
    "# Generate names\n",
    "names = ['Nicholas', 'Ashley', 'Elizabeth', 'Emily']\n",
    "\n",
    "# Define the distribution of random integers\n",
    "# 85% of random integers correspond to the first name\n",
    "# 15% of random integers are evenly distributed across the other three names\n",
    "# Generate random integers according to the specified distribution\n",
    "random_ints = np.random.choice(len(names), size=num_records, p=[0.85, 0.05, 0.05, 0.05])\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame({'name': [names[i] for i in random_ints],\n",
    "                   'random_int': np.random.randint(1000, 10000, size=num_records)})\n",
    "\n",
    "spark_df = spark.createDataFrame(df)\n",
    "\n",
    "spark_df.write.partitionBy('name').mode('overwrite').parquet('s3a://spark/big_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcc5bda-c108-4f61-8b16-44a38ebdc438",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Append data to minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d31b506-1d5d-4886-ab22-3deb813f66ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 5/5 [10:56<00:00, 131.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 23s, sys: 10.1 s, total: 7min 33s\n",
      "Wall time: 10min 56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_records = 10**7\n",
    "\n",
    "names = ['Nicholas', 'Ashley', 'Elizabeth', 'Emily']\n",
    "\n",
    "# Generate random integers according to the specified distribution\n",
    "random_ints = np.random.choice(len(names), size=num_records, p=[0.85, 0.05, 0.05, 0.05])\n",
    "\n",
    "for _ in tqdm(range(5)):\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame({'name': [names[i] for i in random_ints],\n",
    "                       'random_int': np.random.randint(1, 1000000, size=num_records)})\n",
    "    \n",
    "    spark.createDataFrame(df) \\\n",
    "        .write \\\n",
    "        .partitionBy('name') \\\n",
    "        .mode('append') \\\n",
    "        .parquet('s3a://spark/big_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452939a9-96f9-4a41-a1f1-260c2731c3dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Read tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18f72fa3-af34-4081-9478-cfd62f673714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|     name|some_flag|\n",
      "+---------+---------+\n",
      "| Nicholas|        1|\n",
      "|   Ashley|        2|\n",
      "|    Emily|        3|\n",
      "|Elizabeth|        4|\n",
      "+---------+---------+\n",
      "\n",
      "CPU times: user 4.73 ms, sys: 7.95 ms, total: 12.7 ms\n",
      "Wall time: 23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "big_table = spark.read.parquet('s3a://spark/big_table')\n",
    "\n",
    "little_table = big_table \\\n",
    "    .select('name') \\\n",
    "    .distinct() \\\n",
    "    .withColumn('some_flag', \n",
    "        F.when(F.col('name') == 'Nicholas', 1) \\\n",
    "         .when(F.col('name') == 'Ashley', 2) \\\n",
    "         .when(F.col('name') == 'Emily', 3) \\\n",
    "         .when(F.col('name') == 'Elizabeth', 4) \\\n",
    "         .otherwise(0)\n",
    "    )\n",
    "\n",
    "little_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3e511a-f0d1-4cab-83ac-3c27db2a5cef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Join (cache and broadcast hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcec699-4264-4855-ac61-fe2021ce084e",
   "metadata": {},
   "source": [
    "##### Raw join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6334cc3-df1b-4964-aa01-ccf86aee16f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = make_spark_session()\n",
    "\n",
    "big_table = spark.read.parquet('s3a://spark/big_table')\n",
    "\n",
    "little_table = big_table \\\n",
    "    .select('name') \\\n",
    "    .distinct() \\\n",
    "    .withColumn('some_flag', \n",
    "        F.when(F.col('name') == 'Nicholas', 1) \\\n",
    "         .when(F.col('name') == 'Ashley', 2) \\\n",
    "         .when(F.col('name') == 'Emily', 3) \\\n",
    "         .when(F.col('name') == 'Elizabeth', 4) \\\n",
    "         .otherwise(0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8309b13a-cab6-4888-a669-bc641cc03afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [name#1], [name#10], Inner\n",
      "   :- Sort [name#1 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(name#1, 200), ENSURE_REQUIREMENTS, [plan_id=23]\n",
      "   :     +- FileScan parquet [random_int#0L,name#1] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://spark/big_table], PartitionFilters: [isnotnull(name#1)], PushedFilters: [], ReadSchema: struct<random_int:bigint>\n",
      "   +- Sort [name#10 ASC NULLS FIRST], false, 0\n",
      "      +- HashAggregate(keys=[name#10], functions=[])\n",
      "         +- Exchange hashpartitioning(name#10, 200), ENSURE_REQUIREMENTS, [plan_id=19]\n",
      "            +- HashAggregate(keys=[name#10], functions=[])\n",
      "               +- FileScan parquet [name#10] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://spark/big_table], PartitionFilters: [isnotnull(name#10)], PushedFilters: [], ReadSchema: struct<>\n",
      "\n",
      "\n",
      "+----------+--------+--------+---------+\n",
      "|random_int|name    |name    |some_flag|\n",
      "+----------+--------+--------+---------+\n",
      "|344096    |Nicholas|Nicholas|1        |\n",
      "|719897    |Nicholas|Nicholas|1        |\n",
      "|559561    |Nicholas|Nicholas|1        |\n",
      "|260272    |Nicholas|Nicholas|1        |\n",
      "|393758    |Nicholas|Nicholas|1        |\n",
      "|186294    |Nicholas|Nicholas|1        |\n",
      "|262016    |Nicholas|Nicholas|1        |\n",
      "|60167     |Nicholas|Nicholas|1        |\n",
      "|828440    |Nicholas|Nicholas|1        |\n",
      "|198249    |Nicholas|Nicholas|1        |\n",
      "+----------+--------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 9.29 ms, sys: 1.7 ms, total: 11 ms\n",
      "Wall time: 21.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "joined_v0 = big_table.join(little_table, on=[big_table.name == little_table.name], how='inner')\n",
    "\n",
    "joined_v0.explain()\n",
    "\n",
    "joined_v0.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67960ebf-5943-46c0-bf56-47b1f1e2a9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e806de5-3d5c-4a7c-9aca-a5d6765ea06c",
   "metadata": {},
   "source": [
    "##### Repartitioned data join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f97ee9b3-70c7-4981-83d8-9afe2ecedc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = make_spark_session()\n",
    "\n",
    "big_table = spark.read.parquet('s3a://spark/big_table').orderBy('name').repartition('name')\n",
    "\n",
    "little_table = big_table \\\n",
    "    .select('name') \\\n",
    "    .distinct() \\\n",
    "    .withColumn('some_flag', \n",
    "        F.when(F.col('name') == 'Nicholas', 1) \\\n",
    "         .when(F.col('name') == 'Ashley', 2) \\\n",
    "         .when(F.col('name') == 'Emily', 3) \\\n",
    "         .when(F.col('name') == 'Elizabeth', 4) \\\n",
    "         .otherwise(0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a5373c8-ce1b-4814-8fd0-605362919891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [name#38], [name#47], Inner\n",
      "   :- Sort [name#38 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(name#38, 200), REPARTITION_BY_COL, [plan_id=196]\n",
      "   :     +- FileScan parquet [random_int#37L,name#38] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://spark/big_table], PartitionFilters: [isnotnull(name#38)], PushedFilters: [], ReadSchema: struct<random_int:bigint>\n",
      "   +- Sort [name#47 ASC NULLS FIRST], false, 0\n",
      "      +- HashAggregate(keys=[name#47], functions=[])\n",
      "         +- HashAggregate(keys=[name#47], functions=[])\n",
      "            +- Exchange hashpartitioning(name#47, 200), REPARTITION_BY_COL, [plan_id=198]\n",
      "               +- FileScan parquet [name#47] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://spark/big_table], PartitionFilters: [isnotnull(name#47)], PushedFilters: [], ReadSchema: struct<>\n",
      "\n",
      "\n",
      "+----------+------+------+---------+\n",
      "|random_int|name  |name  |some_flag|\n",
      "+----------+------+------+---------+\n",
      "|688827    |Ashley|Ashley|2        |\n",
      "|932044    |Ashley|Ashley|2        |\n",
      "|682022    |Ashley|Ashley|2        |\n",
      "|227875    |Ashley|Ashley|2        |\n",
      "|201969    |Ashley|Ashley|2        |\n",
      "|948800    |Ashley|Ashley|2        |\n",
      "|570760    |Ashley|Ashley|2        |\n",
      "|783424    |Ashley|Ashley|2        |\n",
      "|17400     |Ashley|Ashley|2        |\n",
      "|857095    |Ashley|Ashley|2        |\n",
      "+----------+------+------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 8.36 ms, sys: 1.62 ms, total: 9.97 ms\n",
      "Wall time: 23.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "joined_v1 = big_table.join(little_table, on=[big_table.name == little_table.name], how='inner')\n",
    "\n",
    "joined_v1.explain()\n",
    "\n",
    "joined_v1.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "191f1744-0fbc-4e8f-a498-2879308984b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f708b16-10a0-4ea1-ae04-b24a911b4af8",
   "metadata": {},
   "source": [
    "##### Repartitioned and sorted data join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d28d2b25-3350-42c5-bb46-909c6ed37eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = make_spark_session()\n",
    "\n",
    "big_table = spark.read.parquet('s3a://spark/big_table').orderBy('name').repartition('name')\n",
    "\n",
    "little_table = big_table \\\n",
    "    .select('name') \\\n",
    "    .distinct() \\\n",
    "    .withColumn('some_flag', \n",
    "        F.when(F.col('name') == 'Nicholas', 1) \\\n",
    "         .when(F.col('name') == 'Ashley', 2) \\\n",
    "         .when(F.col('name') == 'Emily', 3) \\\n",
    "         .when(F.col('name') == 'Elizabeth', 4) \\\n",
    "         .otherwise(0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d9699ec-79ce-4aa9-a737-b41a59687a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [name#75], [name#84], Inner\n",
      "   :- Sort [name#75 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(name#75, 200), REPARTITION_BY_COL, [plan_id=371]\n",
      "   :     +- FileScan parquet [random_int#74L,name#75] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://spark/big_table], PartitionFilters: [isnotnull(name#75)], PushedFilters: [], ReadSchema: struct<random_int:bigint>\n",
      "   +- Sort [name#84 ASC NULLS FIRST], false, 0\n",
      "      +- HashAggregate(keys=[name#84], functions=[])\n",
      "         +- HashAggregate(keys=[name#84], functions=[])\n",
      "            +- Exchange hashpartitioning(name#84, 200), REPARTITION_BY_COL, [plan_id=373]\n",
      "               +- FileScan parquet [name#84] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://spark/big_table], PartitionFilters: [isnotnull(name#84)], PushedFilters: [], ReadSchema: struct<>\n",
      "\n",
      "\n",
      "+----------+------+------+---------+\n",
      "|random_int|name  |name  |some_flag|\n",
      "+----------+------+------+---------+\n",
      "|419516    |Ashley|Ashley|2        |\n",
      "|161327    |Ashley|Ashley|2        |\n",
      "|939775    |Ashley|Ashley|2        |\n",
      "|819526    |Ashley|Ashley|2        |\n",
      "|430678    |Ashley|Ashley|2        |\n",
      "|567342    |Ashley|Ashley|2        |\n",
      "|908851    |Ashley|Ashley|2        |\n",
      "|609366    |Ashley|Ashley|2        |\n",
      "|360175    |Ashley|Ashley|2        |\n",
      "|42356     |Ashley|Ashley|2        |\n",
      "+----------+------+------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 5.57 ms, sys: 3.85 ms, total: 9.42 ms\n",
      "Wall time: 23.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "joined_v1 = big_table.join(little_table, on=[big_table.name == little_table.name], how='inner')\n",
    "\n",
    "joined_v1.explain()\n",
    "\n",
    "joined_v1.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f0308bd-eaf1-438d-83a5-23a452fdb121",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c4e05a-9009-45b6-a4ec-e17b69e18554",
   "metadata": {},
   "source": [
    "##### Repartitioned and cached data join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39099d3c-5960-452f-a94f-dbef539e9f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = make_spark_session()\n",
    "\n",
    "big_table = spark.read.parquet('s3a://spark/big_table').repartition('name')\n",
    "big_table.cache()\n",
    "big_table.count()\n",
    "\n",
    "little_table = big_table \\\n",
    "    .select('name') \\\n",
    "    .distinct() \\\n",
    "    .withColumn('some_flag', \n",
    "        F.when(F.col('name') == 'Nicholas', 1) \\\n",
    "         .when(F.col('name') == 'Ashley', 2) \\\n",
    "         .when(F.col('name') == 'Emily', 3) \\\n",
    "         .when(F.col('name') == 'Elizabeth', 4) \\\n",
    "         .otherwise(0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44382fb4-ae00-4268-b45b-69e4a872f510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "little_table.cache()\n",
    "little_table.count()\n",
    "little_table.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45b2bc2e-c870-4e5f-9314-d9df85d222cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [name#112], [name#295], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(name#112)\n",
      "   :  +- InMemoryTableScan [random_int#111L, name#112], [isnotnull(name#112)]\n",
      "   :        +- InMemoryRelation [random_int#111L, name#112], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :              +- Exchange hashpartitioning(name#112, 200), REPARTITION_BY_COL, [plan_id=538]\n",
      "   :                 +- *(1) ColumnarToRow\n",
      "   :                    +- FileScan parquet [random_int#111L,name#112] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://spark/big_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<random_int:bigint>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=639]\n",
      "      +- Filter isnotnull(name#295)\n",
      "         +- InMemoryTableScan [name#295, some_flag#184], [isnotnull(name#295)]\n",
      "               +- InMemoryRelation [name#295, some_flag#184], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                     +- *(1) HashAggregate(keys=[name#112], functions=[])\n",
      "                        +- *(1) HashAggregate(keys=[name#112], functions=[])\n",
      "                           +- InMemoryTableScan [name#112]\n",
      "                                 +- InMemoryRelation [random_int#111L, name#112], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                                       +- Exchange hashpartitioning(name#112, 200), REPARTITION_BY_COL, [plan_id=538]\n",
      "                                          +- *(1) ColumnarToRow\n",
      "                                             +- FileScan parquet [random_int#111L,name#112] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://spark/big_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<random_int:bigint>\n",
      "\n",
      "\n",
      "+----------+------+------+---------+\n",
      "|random_int|name  |name  |some_flag|\n",
      "+----------+------+------+---------+\n",
      "|419516    |Ashley|Ashley|2        |\n",
      "|161327    |Ashley|Ashley|2        |\n",
      "|939775    |Ashley|Ashley|2        |\n",
      "|819526    |Ashley|Ashley|2        |\n",
      "|430678    |Ashley|Ashley|2        |\n",
      "|567342    |Ashley|Ashley|2        |\n",
      "|908851    |Ashley|Ashley|2        |\n",
      "|609366    |Ashley|Ashley|2        |\n",
      "|360175    |Ashley|Ashley|2        |\n",
      "|42356     |Ashley|Ashley|2        |\n",
      "+----------+------+------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 5.9 ms, sys: 0 ns, total: 5.9 ms\n",
      "Wall time: 4.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "joined_v1 = big_table.join(little_table, on=[big_table.name == little_table.name], how='inner')\n",
    "\n",
    "joined_v1.explain()\n",
    "\n",
    "joined_v1.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4920aef-a356-4584-b315-945399144789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.68 ms, sys: 2.5 ms, total: 5.17 ms\n",
      "Wall time: 12 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "big_table.unpersist()\n",
    "big_table.count()\n",
    "big_table.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4befed8-f755-44bc-a0fb-941df96b959d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "little_table.unpersist()\n",
    "little_table.count()\n",
    "little_table.is_cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53f02f7f-6a7b-4df5-bc35-9c559d7d9822",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd93fe5-8923-4946-9ee3-b7339757dd24",
   "metadata": {},
   "source": [
    "##### Sorted, repartitioned, cached data join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f9f829c-88ab-4ce2-bb5b-1515ccffd3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.2 ms, sys: 17 ms, total: 51.2 ms\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "spark = make_spark_session()\n",
    "\n",
    "big_table = spark.read.parquet('s3a://spark/big_table').orderBy('name').repartition('name')\n",
    "big_table.cache()\n",
    "big_table.count()\n",
    "\n",
    "little_table = big_table \\\n",
    "    .select('name') \\\n",
    "    .distinct() \\\n",
    "    .withColumn('some_flag', \n",
    "        F.when(F.col('name') == 'Nicholas', 1) \\\n",
    "         .when(F.col('name') == 'Ashley', 2) \\\n",
    "         .when(F.col('name') == 'Emily', 3) \\\n",
    "         .when(F.col('name') == 'Elizabeth', 4) \\\n",
    "         .otherwise(0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4bbb5bb-cc07-4008-a07a-dbab9e6ea51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [name#487], [name#563], Inner\n",
      "   :- Sort [name#487 ASC NULLS FIRST], false, 0\n",
      "   :  +- Filter isnotnull(name#487)\n",
      "   :     +- InMemoryTableScan [random_int#486L, name#487], [isnotnull(name#487)]\n",
      "   :           +- InMemoryRelation [random_int#486L, name#487], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :                 +- Exchange hashpartitioning(name#487, 200), REPARTITION_BY_COL, [plan_id=896]\n",
      "   :                    +- *(1) ColumnarToRow\n",
      "   :                       +- FileScan parquet [random_int#486L,name#487] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://spark/big_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<random_int:bigint>\n",
      "   +- Sort [name#563 ASC NULLS FIRST], false, 0\n",
      "      +- HashAggregate(keys=[name#563], functions=[])\n",
      "         +- HashAggregate(keys=[name#563], functions=[])\n",
      "            +- Filter isnotnull(name#563)\n",
      "               +- InMemoryTableScan [name#563], [isnotnull(name#563)]\n",
      "                     +- InMemoryRelation [random_int#562L, name#563], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                           +- Exchange hashpartitioning(name#487, 200), REPARTITION_BY_COL, [plan_id=896]\n",
      "                              +- *(1) ColumnarToRow\n",
      "                                 +- FileScan parquet [random_int#486L,name#487] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://spark/big_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<random_int:bigint>\n",
      "\n",
      "\n",
      "+----------+------+------+---------+\n",
      "|random_int|name  |name  |some_flag|\n",
      "+----------+------+------+---------+\n",
      "|925735    |Ashley|Ashley|2        |\n",
      "|745991    |Ashley|Ashley|2        |\n",
      "|786073    |Ashley|Ashley|2        |\n",
      "|794752    |Ashley|Ashley|2        |\n",
      "|858402    |Ashley|Ashley|2        |\n",
      "|316352    |Ashley|Ashley|2        |\n",
      "|767086    |Ashley|Ashley|2        |\n",
      "|961407    |Ashley|Ashley|2        |\n",
      "|917718    |Ashley|Ashley|2        |\n",
      "|657457    |Ashley|Ashley|2        |\n",
      "+----------+------+------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 10.9 ms, sys: 0 ns, total: 10.9 ms\n",
      "Wall time: 31.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "joined_v1 = big_table.join(little_table, on=[big_table.name == little_table.name], how='inner')\n",
    "\n",
    "joined_v1.explain()\n",
    "\n",
    "joined_v1.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745aa918-3095-49ec-b122-18856c699410",
   "metadata": {},
   "source": [
    "##### Broadcast hint works!\n",
    "\n",
    "Here two identical operations with and without hint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cfabfd0-061e-453c-8580-370ad8c532b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [name#487], [name#1103], Inner\n",
      "   :- Sort [name#487 ASC NULLS FIRST], false, 0\n",
      "   :  +- Filter isnotnull(name#487)\n",
      "   :     +- InMemoryTableScan [random_int#486L, name#487], [isnotnull(name#487)]\n",
      "   :           +- InMemoryRelation [random_int#486L, name#487], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :                 +- Exchange hashpartitioning(name#487, 200), REPARTITION_BY_COL, [plan_id=896]\n",
      "   :                    +- *(1) ColumnarToRow\n",
      "   :                       +- FileScan parquet [random_int#486L,name#487] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://spark/big_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<random_int:bigint>\n",
      "   +- Sort [name#1103 ASC NULLS FIRST], false, 0\n",
      "      +- HashAggregate(keys=[name#1103], functions=[])\n",
      "         +- HashAggregate(keys=[name#1103], functions=[])\n",
      "            +- Filter isnotnull(name#1103)\n",
      "               +- InMemoryTableScan [name#1103], [isnotnull(name#1103)]\n",
      "                     +- InMemoryRelation [random_int#1102L, name#1103], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                           +- Exchange hashpartitioning(name#487, 200), REPARTITION_BY_COL, [plan_id=896]\n",
      "                              +- *(1) ColumnarToRow\n",
      "                                 +- FileScan parquet [random_int#486L,name#487] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://spark/big_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<random_int:bigint>\n",
      "\n",
      "\n",
      "+----------+------+------+---------+\n",
      "|random_int|name  |name  |some_flag|\n",
      "+----------+------+------+---------+\n",
      "|688827    |Ashley|Ashley|2        |\n",
      "|932044    |Ashley|Ashley|2        |\n",
      "|682022    |Ashley|Ashley|2        |\n",
      "|227875    |Ashley|Ashley|2        |\n",
      "|201969    |Ashley|Ashley|2        |\n",
      "|948800    |Ashley|Ashley|2        |\n",
      "|570760    |Ashley|Ashley|2        |\n",
      "|783424    |Ashley|Ashley|2        |\n",
      "|17400     |Ashley|Ashley|2        |\n",
      "|857095    |Ashley|Ashley|2        |\n",
      "+----------+------+------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 5.34 ms, sys: 4.22 ms, total: 9.55 ms\n",
      "Wall time: 26.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "joined_v1 = big_table.join(little_table, on=[big_table.name == little_table.name], how='inner')\n",
    "\n",
    "joined_v1.explain()\n",
    "\n",
    "joined_v1.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2cd979c-8145-4024-8554-f43dc40c0a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [name#487], [name#1449], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(name#487)\n",
      "   :  +- InMemoryTableScan [random_int#486L, name#487], [isnotnull(name#487)]\n",
      "   :        +- InMemoryRelation [random_int#486L, name#487], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "   :              +- Exchange hashpartitioning(name#487, 200), REPARTITION_BY_COL, [plan_id=896]\n",
      "   :                 +- *(1) ColumnarToRow\n",
      "   :                    +- FileScan parquet [random_int#486L,name#487] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://spark/big_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<random_int:bigint>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=1432]\n",
      "      +- HashAggregate(keys=[name#1449], functions=[])\n",
      "         +- HashAggregate(keys=[name#1449], functions=[])\n",
      "            +- Filter isnotnull(name#1449)\n",
      "               +- InMemoryTableScan [name#1449], [isnotnull(name#1449)]\n",
      "                     +- InMemoryRelation [random_int#1448L, name#1449], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                           +- Exchange hashpartitioning(name#487, 200), REPARTITION_BY_COL, [plan_id=896]\n",
      "                              +- *(1) ColumnarToRow\n",
      "                                 +- FileScan parquet [random_int#486L,name#487] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[s3a://spark/big_table], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<random_int:bigint>\n",
      "\n",
      "\n",
      "+----------+------+------+---------+\n",
      "|random_int|name  |name  |some_flag|\n",
      "+----------+------+------+---------+\n",
      "|688827    |Ashley|Ashley|2        |\n",
      "|932044    |Ashley|Ashley|2        |\n",
      "|682022    |Ashley|Ashley|2        |\n",
      "|227875    |Ashley|Ashley|2        |\n",
      "|201969    |Ashley|Ashley|2        |\n",
      "|948800    |Ashley|Ashley|2        |\n",
      "|570760    |Ashley|Ashley|2        |\n",
      "|783424    |Ashley|Ashley|2        |\n",
      "|17400     |Ashley|Ashley|2        |\n",
      "|857095    |Ashley|Ashley|2        |\n",
      "+----------+------+------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 6.82 ms, sys: 1.94 ms, total: 8.76 ms\n",
      "Wall time: 9.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "joined_v1 = big_table.join(little_table.hint(\"broadcast\"), on=[big_table.name == little_table.name], how='inner')\n",
    "\n",
    "joined_v1.explain()\n",
    "\n",
    "joined_v1.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac48d8f9-a292-41be-835a-577635e3dd75",
   "metadata": {},
   "source": [
    "### Salt join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb8a336-8598-4908-8761-4448d2e19704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b350a91-323a-40e3-ab6a-0cf9a6208798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
